\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage[normalem]{ulem}
\usepackage{fancyhdr,graphicx,amsmath,amssymb, mathtools, scrextend, titlesec, enumitem}
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage{parskip}
\usepackage{url}

\title{Towards the Age of Fast Programmable Networks}
\author{Yanfang Le}
\date{}

\begin{document}
\maketitle
\vspace{-5mm}
There has been a recent emergence of big data systems, such as MapReduce and Spark for data analytics and TensorFlow and PyTorch for machine learning.
While these frameworks are compute and memory-intensive, they also place high demands on the network for distributing data. 
However, current trends show that Ethernet network speed are outgrowing CPU processing power and memory bandwidth, which are becoming  bottlenecks~\cite{CPU:bandwidth,DRAM:scaling}. 
This provides opportunities to innovate with new application designs, such as datacenter resource disaggregation~\cite{aggregation:gao}, and poses new challenges for existing applications that must be re-designed to fully utilize high-speed networks. In addition, {\em network offloading} to programmable hardware is a 
promising approach to help relieve packet processing pressure on CPU. For example, RDMA offloads packetization to NICs, and smart NICS (sNICs) can run network functions~\cite{uno:le}.
%remote memory access can be offloaded to the programmable switch by NetCache~\cite{Jin:2017:netcache}; and computations get offloaded to the programmable switch by SwitchML~\cite{switchml}. 
%and provide a faster network by offloading packet processing functions to Network Interface Cards (NICs).
%Advanced NICs equipped with special hardware (ASIC) are purpose-built to offload pre-defined packet processing functions (e.g., OVS fastpath, packet and flow steering). Some NICs can be enhanced with FPGAs for custom function development. 
%Examples include offloading distributed consensus protocols, memcached and key-value stores, rate-limiting packet flows, cryptography, quality of service and storage networking.
%More advanced NICs, i.e., smart NICs (sNICs), which are equipped with fully programmable, system-on-chip-multi-core processor, can run a full-fledged operating system and run any arbitrary packet processing function.
This advanced hardware is increasingly being deployed in data centers %to help reduce the host CPU utilization and 
to enable low-latency and high-throughput networks~\cite{Guo:2016:ROC, Firestone:2018:AAN, brainwave}.
These designs provide new opportunities not only to optimize the existing network stacks and innovate with 
new networking stacks (network-assisted protocols), 
but also to optimize the performance of distributed systems by processing traffic inside the network (network-assisted applications).
%However, it brings new challenges to the data center network if not properly used. For example, RoCE, a NIC offloaded network protocol that allows remote direct memory access (RDMA) over Ethernet network, requires the lossless network. Design choices including which RDMA operations to use and how to use them can significantly affect performance of RDMA applications. 
%The introduction of smart NICs to virtual networking can impose additional complexity in the existing management/control planes. The inefficiency of a single core of smart NIC compared with x86 CPU core makes us think about what kind of network functions should be offloaded to the smart NICs. 
My current research focuses on better utilizing the aforementioned programmable hardware to improve the network performance. 
\section*{Past and Current Projects}
I have one on-going project (Gazelle) and two completed projects (RoGUE and UNO).

\textbf{Gazelle: Towards Bounded Low Latency in High-speed Networks}~\cite{Gazelle}\textbf{.}
Modern applications and cloud services are severely impacted by high tail
latency.  However, bounding tail latency at high loads is a
significant challenge, which needs to be solved across all infrastructure
components, including datacenter networks.  In this project, we focus on
providing bounded low network delays for application level
messages in high-speed networks. 
We design a network architecture, Gazelle, that employs
a global hardware packet scheduler at programmable switches.
The scheduler decides when to send packets into the network for each host. 
Our goal is to realize a system, which provides low latency in a lossless manner
but does not sacrifice network utilization. 
This work is ongoing.

\textbf{RoGUE: RDMA over Generic Unconverged Ethernet}~
\cite{Le:2018:rogue}\textbf{.} Current RDMA over Converged Ethernet (RoCE) requires Priority Flow
Control (PFC) that uses backpressure-based congestion
control to offer RDMA a lossless abstraction. Unfortunately,
PFC suffers from a series of problems, e.g., deadlock, head of line blocking, and unfairness. 
As a result,
RoCE’s adoption has been slow and requires complex
network management. Several efforts, such as TIMELY~\cite{Mittal:2015:TRC}
and DCQCN~\cite{Zhu:2015:CCL}, have addressed some of the issues by reducing
the generation of PFC frames, but they do not completely
solve the problem.
We designed RoGUE, a new congestion control and
recovery mechanism for RDMA over Ethernet that does
not rely on PFC. RoGUE does judicious labor division 
between hardware and software for congestion control, flow control and loss recovery.  
Using congestion windows and RTT-based
congestion signals inspired by TCP Vegas and
TIMELY, RoGUE allows use of RDMA for high performance
without special network configuration (i.e., no PFC).
%In addition, RoGUE supports both the RC and UC RDMA transports.
Our experiments show that RoGUE achieves
performance and CPU utilization close to native RDMA
protocols while gracefully tolerating congested networks.

\textbf{UNO: Unifying Host and Smart NIC Offload for Flexible Packet
Processing}~\cite{uno:le}\textbf{.} Increasingly, Smart NICs (sNICs) are being used
in data centers to offload networking functions (NFs) from host
processors thereby making these processors available for tenant applications.
Modern sNICs have fully programmable, energy-efficient
multi-core processors and can run many packet processing functions,
including a full-blown programmable switch. However, 
having multiple switch instances deployed across the host hypervisor
and the attached sNICs complicates control operations and makes
data-plane operations more complex.
We designed UNO, a generalized NF offload
architecture. It can transparently offload selected packet processing functions from host processors to sNICs while keeping the data center-wide
network control and management planes unmodified. UNO
exposes a single virtual control plane to the data center-wide network controller and
hides dynamic NF offload behind a unified virtual management
plane. This enables UNO to make optimal use of host’s and sNIC’s
combined packet processing capabilities with local optimization
based on locally observed traffic patterns and resource consumption, 
without involving any central controller. Our experiments based 
on the UNO prototype in realistic scenarios show promising
results.
%: it can save processing worth up to 8 CPU cores, reduce power usage by up to 2x, and reduce the control plane overhead by more than 50\%.


\section*{Future Research}
I believe there will be further tremendous opportunities to better utilize the new features of the programmable NICs and switches. Beyond that, I am interested in exploring the opportunities and challenges for the applications in the age of fast networks. 
%%\textbf{Scalable RDMA:}
%%Due to the limitation of NIC cache, too many outstanding requests and/or queue pairs issued in a RDMA-capable NIC can lead to poor performance of RDMA applications~\cite{farm:aleksandar}. UD transport in RDMA protocol allows one queue at an end host can connect to multiple end hosts, which provides a possibility for RDMA to solve the scalability issues. There are two main challenges here: a) Only one RDMA verb, SEND, provides UD transport and SEND UD is a two-sided RDMA, which requires CPU involvement at local and remote hosts. Making SEND UD achieve a comparable CPU utilization as one-sided RDMA at local and remote hosts could be a big challenge. b) UD does not provide ACK of a packet, i.e., it does not guarantee reliability as the RDMA RC transport does. Reliability is too complex for the current commodity NIC to handle if it is implemented in hardware. Software-based reliability requires high CPU utilization. How to handle the trade-off between the reliability at the software and CPU utilization is still an open question.

%%
%%\textbf{Rethinking TCP Protocol:} 
%%Recent works have shown that TCP works worse than RDMA in terms of performance, i.e., TCP has a higher latency, lower throughput, higher CPU utilization than RDMA. Due to the simplicity of the RDMA protocol, it can be fully offloaded to the hardware. It also assumes that the network is lossless, which is achieved by PFC and PFC has well known troubles in network stability. The inefficiency of iWARP~\cite{iWARP:analysis}, the full offloading version of TCP, indicates that we should properly place reliability and congestion control functionality at hardware and software.
%%I am seeking an approach that can handle congestion control and reliability either in hardware or software but with comparable performance as RDMA, i.e., high throughput (100Gbps or higher), low latency ($\mu$s or $ns$), low CPU utilization.
%%In this context, congestion control algorithms and packet loss recovery mechanisms should be re-designed to fit into the fast network.

\textbf{Network-Assisted Applications}
Network-Assisted applications, e.g., SwitchML, NetCache,  
have emerged due to the recent advances of programmable
switches. SwitchML uses the computation capacity of programmable switches 
to eliminate  incasts, while NetCache reduces the latencies of RPC requests by using limited switch memory as a cache.
However, these systems assume that the whole network is rack-scale and owned by these applications.
This results in simple and ad hoc transport protocols. To enable general applications to use switch resources, a generic network transport layer is required. The traditional reliable network stack (TCP/IP) does not support this scenario, as it relies on an end-to-end stateful connection that is broken by having some requests handled at switches within the network. A new transport layer, co-designed with switch and host software, can provide better performance and reliability. The main challenges involved here are: {\it How can we design congestion control algorithms to ensure reliability between the end hosts and the switches? How can we support different switch resource allocation policies among different applications or tenants on switches with limited programmability?}

\textbf{Network-Assisted Protocols}
Programmable network hardware provides an opportunity to rethink network protocol design
for datacenters based on a co-design of software on programmable switches and end hosts.
Gazelle is a first example of this, targeting at guaranteed low latency.
This idea can be further extended to other properties such as reliability.
In RoCE, PFC is essential to provide reliability at the cost of compromising network stability. 
Because of PFC, RoCE NIC implements a simplistic go-back-n mechanism to handle packet loss.
However, standard TCP/IP stacks implement selective retransmission and selective ACKs to handle packet loss more efficiently, but still suffer from  severe performance degradation when timeouts happen.
%Due to the presence of PFC, RoCE NIC only implements a go-back-n mechanism to handle packet loss.
%Because PFC compromises with network stability, data center providers are reluctant to enable PFC. 
%One way to remove PFC is to implement selective retransmission and selective ACKs,
%similar as TCP, in NIC~\cite{irn}, which requires us to work with the NIC vendor. 
We believe programmable switches can do better: these switches can detect packet loss due to the buffer overflow and send NACK packets to trigger fast retransmission, which mitigates the performance cost of go-back-n and timeouts.
These are the open questions: {\it How to suppress the NACK packets sent by the end host? How to minimize the state maintaining in the switch to generate the NACK? Is it possible to recover packet loss from switches instead of the end host? 
Is it possible to extend switch capability to be able to detect packets loss for any reasons? }  

\textbf{Impacts of Faster Network:}
When the network becomes faster, i.e., the bandwidth can reach 100Gbps or higher, and the latency is in the range of 1$\mu$s - 5$\mu$s, there is an open question: \textbf{\it what can we do with the faster network?} One candidate could be intra datacenter shared memory systems. As the bandwidth gap between the network and memory subsystem has shrunk in recent years~\cite{memory:Marcos}, it is feasible to treat remote memory as local memory. Despite the recent efforts of resource disaggregation~\cite{aggregation:gao}, and remote memory~\cite{memory:Marcos, Infiniswap}, 
%and Infiniswap~\cite{Infiniswap}, 
many questions remain unanswered: {\it how to select the best remote host given unstable network conditions? How to recover from a remote host failure? How can we achieve minimum impact on the applications running in the remote host? What kind of applications are suitable to run on these shared memory infrastructures?}

Fast programmable network hardware is a promising approach to reduce host CPU utilization and increase the performance of networks and applications. I am excited to solve practical problems in this area and I believe my research can contribute to improving the performance of data center networks and applications running on these networks. 
\bibliographystyle{plain}
%\bibliography{sample-bibliography}
\bibliography{refer}
\end{document}
